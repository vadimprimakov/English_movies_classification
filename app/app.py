import streamlit as st
from srt_procesing import sub_processing
from catboost import CatBoostClassifier, Pool
import pandas as pd
import nltk
import pickle
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('stopwords')


print('Start app')
df_words = pd.read_csv('app/oxford_dikt.csv')
df_idioms = pd.read_csv('app/theidioms_com.csv', sep='#')
model = CatBoostClassifier()
#PATH_DATA_LOCAL = '/Users/vadimprimakov/Documents/Yandex_practicum/English_movies_classification/app/'
#PATH_DATA_REMOTE = 'Streamlit_app/'

#def load_model(model_name):
    
#    file_local = f'{PATH_DATA_LOCAL}{model_name}'
#    file_remote = f'{PATH_DATA_REMOTE}{model_name}'
    
#    if os.path.isfile(file_local):
#        with open(file_local, 'rb') as file:
#            model = pickle.load(file)
#    else:
#        with open(file_remote, 'rb') as file:
#            model = pickle.load(file)
        
#    return model
#model = load_model(catboostclassifier_model.pkl)
model.load_model('app/catboostclassifier_model.cbm')
features = ['phrases_lenght', 
        'B2', 
        'coleman_liau_index', 
        'word_persentence', 
        'gulpease_index', 
        'gerund_persentence', 
        'words_unique_perphrase', 
        'words_unique_persecond',
        'phrases_count',
        'avg_dificulty',
        'idioms_persentence']

class f:
    BOLD = "\033[1m"
    ITALIC = "\033[3m"
    END = "\033[0m"

TITLE = '–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É—Ä–æ–≤–Ω—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –ø–æ —Å—É–±—Ç–∏—Ç—Ä–∞–º'
st.set_page_config(
                   page_title=TITLE,
                   page_icon='üé¨',
                   initial_sidebar_state='expanded',
                 )
st.title(TITLE)
st.write('–î–∞–Ω–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑—ã–∞–µ—Ç —É—Ä–æ–≤–µ–Ω—å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –¥–ª—è —Ñ–∏–ª—å–º–æ–≤ –∏ —Å–µ—Ä–∏–∞–ª–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ. –í—Å–µ —á—Ç–æ –Ω—É–∂–Ω–æ, —ç—Ç–æ –Ω–∞–π—Ç–∏ —Ñ–∞–π–ª–∏–∫ —Å —Å—É–±—Ç–∏—Ç—Ä–∞–º–∏, –±–µ–∑ —ç—Ç–æ–≥–æ –Ω–∏–∫–∞–∫. –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É—Ä–æ–≤–Ω—è –ø–æ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ CEFR –∏ –º–æ–∂–µ—Ç –±—ã—Ç—å A1, A2, B1, B2, C1, C2.')

upload_file = st.file_uploader('–û—Ç–∫—Ä–æ–π—Ç–µ —Ñ–∞–π–ª —Å—É–±—Ç–∏—Ç–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ .srt', type='srt')

def make_predict(data, model):
    """
    :param data:
    :param model:
    :return:
    """
    predict_pool = Pool(data=data,
                       )
    predict = model.predict(predict_pool)
    decode = {1:'A1',
              2:'A2',
              3:'B1',
              4:'B2',
              5:'C1',
              6:'C2'
             }
    return decode[predict[0][0]]

def make_color(data, model):
    """
    :param data:
    :param model:
    :return:
    """
    predict_pool = Pool(data=data,
                       )
    predict = model.predict(predict_pool)
    if predict == 1 or 2:
        value_color = 'green'
    elif predict == 3 or 4:
        value_color = 'blue'
    elif predict == 5 or 6:
        value_color = 'red'
    return value_color

if upload_file:

    print(upload_file.name)

    df = sub_processing(upload_file, df_words, df_idioms)
    if df is None:
        st.write('–§–∞–π–ª —Å—É–±—Ç–∏—Ç—Ä–æ–≤ –∏–º–µ–µ—Ç –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç')
    else:
        st.header(f'–î–∞–Ω–Ω—ã–π —Ñ–∏–ª—å–º –∏–º–µ–µ—Ç —É—Ä–æ–≤–µ–Ω—å **:{make_color(df[features], model)}[{make_predict(df[features], model)}]** –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ CEFR')

        button = st.button('–ü–æ–∫–∞–∑–∞—Ç—å –∞–Ω–∞–ª–∏–∑')
        if button:

            st.header('–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å–ª–æ–≤ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –û–∫—Å—Ñ–æ—Ä–¥—Å–∫–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è')
            st.bar_chart(df.loc[0, ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']])

            st.write(pd.DataFrame(
                {
                    '–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞':['–ü—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∏–ª—å–º–∞',
                                      '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤',
                                      '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤',
                                      '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–∑',
                                      '–°—Ä–µ–¥–Ω–∏–π —Ç–µ–º–ø —Ä–µ—á–∏',
                                      '–õ–µ–∫—Å–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ',
                                     ],
                    '–ó–Ω–∞—á–µ–Ω–∏–µ':[f'{df.loc[0,"film_lenght"]//3600} —á. {(df.loc[0,"film_lenght"]%3600)//60} –º.',
                                df.loc[0,'words_count'],
                                df.loc[0,'words_unique_count'],
                                df.loc[0,'phrases_count'],
                                f'{round(df.loc[0,"words_count"] / df.loc[0,"film_lenght"] * 60)} —Å–ª/–º–∏–Ω',
                                round(df.loc[0,'lexical_diversity'], 3),
                               ],
                }
            ))

st.caption('------')


st.markdown("–ë–æ–ª—å—à–µ –ø—Ä–æ–µ–∫—Ç–æ–≤ –≤ –ø—Ä–æ—Ñ–∏–ª–µ [GitHub](https://github.com/vadimprimakov)")
st.markdown("–ü–æ –æ—Å—Ç–∞–ª—å–Ω—ã–º –≤–æ–ø—Ä–æ—Å–∞–º [Telegram](https://t.me/vadimprimakov)")
